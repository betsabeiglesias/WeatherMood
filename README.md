# WeatherMood ğŸŒ¦ï¸âœ¨

**WeatherMood** es una aplicaciÃ³n web interactiva que combina informaciÃ³n meteorolÃ³gica en tiempo real con generaciÃ³n de frases motivadoras, basadas en el clima actual de cualquier ubicaciÃ³n seleccionada en el mapa.

---

## ğŸš€ TecnologÃ­as utilizadas

- **Node.js** (servidor backend con Express)
- **Leaflet.js** (mapas interactivos)
- **Tailwind CSS** (estilos rÃ¡pidos y responsivos)
- **FontAwesome** (Ã­conos vectoriales)
- **WeatherAPI** (datos de clima en tiempo real)
- **Ollama** (modelo de lenguaje Llama3 o Salamandra para generaciÃ³n de texto)

---

## âš™ï¸ Requisitos previos

Antes de comenzar asegÃºrate de tener instalado:

- **Node.js** (versiÃ³n recomendada 18+)
- **npm** (gestor de paquetes de Node.js)
- **Ollama** instalado y funcionando localmente

  ğŸ‘‰ Descarga desde: [https://ollama.com/download](https://ollama.com/download)

- Tener un modelo descargado en Ollama (ej: `hdnh2006/salamandra-7b-instruct` o `llama3`)
- Una API Key vÃ¡lida de [WeatherAPI.com](https://www.weatherapi.com/)

---

## ğŸ› ï¸ InstalaciÃ³n y configuraciÃ³n

1. **Clona el repositorio:**

```bash
git clone https://github.com/betsabeiglesias/WeatherMood.git
cd WeatherMood
```

2. **Instala las dependencias:**

```bash
npm install
```

3. **Configura las variables de entorno:**

Crea un archivo `.env` en la raÃ­z del proyecto, basado en `.env.example`, y completa tus datos:

```dotenv
WEATHER_API_KEY=tu_clave_de_weatherapi
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=hdnh2006/salamandra-7b-instruct
```

- `WEATHER_API_KEY`: Tu clave personal de WeatherAPI.
- `OLLAMA_URL`: URL local donde Ollama escucha (por defecto no hay que cambiarla).
- `OLLAMA_MODEL`: Modelo de lenguaje a utilizar.  
  Puedes usar:
  - `hdnh2006/salamandra-7b-instruct` (recomendado y probado).
  - O `llama3` u otro modelo que tengas disponible en Ollama.

> ğŸ’¡ **Nota:** Si usas otro modelo, asegÃºrate de tenerlo descargado en Ollama (`ollama run llama3` o similar).

4. **Inicia el servidor local:**

```bash
npm start
```

Esto levantarÃ¡ un servidor en `http://localhost:3000`.

5. **Inicia Ollama en otra terminal:**

```bash
ollama serve
```

AsegÃºrate de que Ollama estÃ© corriendo en `localhost:11434`.

Luego, carga el modelo si es necesario:

```bash
ollama run salamandra
```

o

```bash
ollama run llama3
```

---

## ğŸŒ Uso

- Abre tu navegador en [http://localhost:3000](http://localhost:3000)
- Haz clic en cualquier punto del mapa interactivo.
- VerÃ¡s:
  - InformaciÃ³n meteorolÃ³gica actualizada del lugar.
  - Una frase inspiradora generada por IA en funciÃ³n del clima.
- TambiÃ©n puedes actualizar la frase pulsando **Actualizar InspiraciÃ³n**.

---

## ğŸ“¦ Estructura del proyecto

```plaintext
WeatherMood/
â”œâ”€â”€ index.html
â”œâ”€â”€ styles.css
â”œâ”€â”€ script.js
â”œâ”€â”€ server.js
â”œâ”€â”€ .env.example
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

---
