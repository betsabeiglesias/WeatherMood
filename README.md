# WeatherMood üå¶Ô∏è‚ú®

**WeatherMood** es una aplicaci√≥n web interactiva que combina informaci√≥n meteorol√≥gica en tiempo real con generaci√≥n de frases motivadoras, basadas en el clima actual de cualquier ubicaci√≥n seleccionada en el mapa.

---

## üöÄ Tecnolog√≠as utilizadas

- **Node.js** (servidor backend con Express)
- **Leaflet.js** (mapas interactivos)
- **Tailwind CSS** (estilos r√°pidos y responsivos)
- **FontAwesome** (√≠conos vectoriales)
- **WeatherAPI** (datos de clima en tiempo real)
- **Ollama** (modelo de lenguaje Llama3 o Salamandra para generaci√≥n de texto)

---

## ‚öôÔ∏è Requisitos previos

Antes de comenzar aseg√∫rate de tener instalado:

- **Node.js** (versi√≥n recomendada 18+)
- **npm** (gestor de paquetes de Node.js)
- **Ollama** instalado y funcionando localmente

  üëâ Descarga desde: [https://ollama.com/download](https://ollama.com/download)

- Tener un modelo descargado en Ollama (ej: `hdnh2006/salamandra-7b-instruct` o `llama3`)
- Una API Key v√°lida de [WeatherAPI.com](https://www.weatherapi.com/)

---

## üõ†Ô∏è Instalaci√≥n y configuraci√≥n

1. **Clona el repositorio:**

```bash
git clone https://github.com/betsabeiglesias/WeatherMood.git
cd WeatherMood
```

2. **Instala las dependencias:**

```bash
npm install
```

3. **Configura las variables de entorno:**

Crea un archivo `.env` en la ra√≠z del proyecto, basado en `.env.example`, y completa tus datos:

```dotenv
WEATHER_API_KEY=tu_clave_de_weatherapi
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=hdnh2006/salamandra-7b-instruct
```

- `WEATHER_API_KEY`: Tu clave personal de WeatherAPI.
- `OLLAMA_URL`: URL local donde Ollama escucha (por defecto no hay que cambiarla).
- `OLLAMA_MODEL`: Modelo de lenguaje a utilizar.  
  Puedes usar:
  - `hdnh2006/salamandra-7b-instruct` (recomendado y probado).
  - O `llama3` u otro modelo que tengas disponible en Ollama.

> üí° **Nota:** Si usas otro modelo, aseg√∫rate de tenerlo descargado en Ollama (`ollama run llama3` o similar).

4. **Inicia el servidor local:**

```bash
npm start
```

Esto levantar√° un servidor en `http://localhost:3000`.

5. **Inicia Ollama en otra terminal:**

```bash
ollama serve
```

Aseg√∫rate de que Ollama est√© corriendo en `localhost:11434`.

Luego, carga el modelo si es necesario:

```bash
ollama run hdnh2006/salamandra-7b-instruct
```

o

```bash
ollama run llama3
```

---

## üåê Uso

- Abre tu navegador en [http://localhost:3000](http://localhost:3000)
- Haz clic en cualquier punto del mapa interactivo.
- Ver√°s:
  - Informaci√≥n meteorol√≥gica actualizada del lugar.
  - Una frase inspiradora generada por IA en funci√≥n del clima.
- Tambi√©n puedes actualizar la frase pulsando **Actualizar Inspiraci√≥n**.

---
